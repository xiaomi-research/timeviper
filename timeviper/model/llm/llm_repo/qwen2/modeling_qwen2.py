#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen2/modular_qwen2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from dataclasses import dataclass
from typing import Callable, List, Optional, Tuple, Union

import torch
from torch import nn
from transformers import AutoConfig, AutoModelForCausalLM
from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import (
    create_causal_mask,
    create_sliding_window_causal_mask,
)
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import (
    GenericForQuestionAnswering,
    GenericForSequenceClassification,
    GenericForTokenClassification,
    GradientCheckpointingLayer,
)
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import (
    ModelOutput,
    TransformersKwargs,
    auto_docstring,
    can_return_tuple,
)
from transformers.utils.deprecation import deprecate_kwarg
from transformers.utils.generic import check_model_inputs

from .configuration_qwen2 import Qwen2Config

# pdrop: Assuming Qwen2VLSdpaCrossAttention is available in the user's environment.
try:
    from .merge_modules.cross_attention import Qwen2VLSdpaCrossAttention
except ImportError:
    print(
        "Warning: Qwen2VLSdpaCrossAttention not found. The pdrop merge functionality will not work without it."
    )
    Qwen2VLSdpaCrossAttention = None


# pdrop: Added for custom model output when labels are modified.
@dataclass
class BaseModelOutputWithPastAndLabels(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[tuple[torch.FloatTensor, ...]] = None
    labels: Optional[torch.FloatTensor] = None


class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(
        query.dtype
    )
    attn_weights = nn.functional.dropout(
        attn_weights, p=dropout, training=module.training
    )
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen2Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(
            config, "head_dim", config.hidden_size // config.num_attention_heads
        )
        self.num_key_value_groups = (
            config.num_attention_heads // config.num_key_value_heads
        )
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True
        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=True
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=False
        )
        self.sliding_window = (
            config.sliding_window
            if config.layer_types[layer_idx] == "sliding_attention"
            else None
        )

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin
        )

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(
                key_states, value_states, self.layer_idx, cache_kwargs
            )

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[
                self.config._attn_implementation
            ]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # main diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


@use_kernel_forward_from_hub("RMSNorm")
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen2DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )
        self.attention_type = config.layer_types[layer_idx]

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[
            tuple[torch.Tensor, torch.Tensor]
        ] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@auto_docstring
class Qwen2PreTrainedModel(PreTrainedModel):
    config: Qwen2Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen2DecoderLayer,
        "attentions": Qwen2Attention,
    }


class Qwen2RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen2Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and isinstance(config.rope_scaling, dict):
            self.rope_type = config.rope_scaling.get(
                "rope_type", config.rope_scaling.get("type")
            )
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = (
            self.inv_freq[None, :, None]
            .float()
            .expand(position_ids.shape[0], -1, 1)
            .to(x.device)
        )
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = (
            x.device.type
            if isinstance(x.device.type, str) and x.device.type != "mps"
            else "cpu"
        )
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (
                inv_freq_expanded.float() @ position_ids_expanded.float()
            ).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class Qwen2Model(Qwen2PreTrainedModel):
    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx
        )
        self.layers = nn.ModuleList(
            [
                Qwen2DecoderLayer(config, layer_idx)
                for layer_idx in range(config.num_hidden_layers)
            ]
        )
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen2RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # for pdrop
        self.use_pdrop = getattr(config, "use_pdrop", False)

        self.pdrop_args = {"use_pdrop": self.use_pdrop}
        if self.use_pdrop:
            assert (
                hasattr(config, "pdrop_type") and config.pdrop_type is not None
            ), "use_pdrop is True, but pdrop_type is not set"
            self.pdrop_types = [t.split("_") for t in config.pdrop_type.split("-")]
            assert all(
                [len(typ) == 3 for typ in self.pdrop_types]
            ), "pdrop_type should be like 'type_layernum_ratio-...' "
            self.pdrop_args.update(
                {
                    "pdrop_compress_types": [typ[0] for typ in self.pdrop_types],
                    "pdrop_layers": [int(typ[1]) for typ in self.pdrop_types],
                    "pdrop_ratios": [1] + [float(typ[2]) for typ in self.pdrop_types],
                }
            )
            print(f"Using pdrop with types: {self.pdrop_types}")

            merge_module_name = getattr(config, "merge_module", "no_merge")
            if merge_module_name == "CrossAttention":
                self.merge_module_names = [
                    "attention" for _ in self.pdrop_args.get("pdrop_layers", [])
                ]
                self.merge_modules = nn.ModuleList(
                    [
                        Qwen2VLSdpaCrossAttention(config, layer_idx=idx)
                        for idx in range(config.num_hidden_layers)
                        if idx in self.pdrop_args.get("pdrop_layers", [])
                    ]
                )
                self.alpha = nn.Parameter((torch.zeros(len(self.merge_modules))))
                self.merge_ffn_modules = nn.ModuleList(
                    [
                        Qwen2MLP(config)
                        for idx in range(config.num_hidden_layers)
                        if idx in self.pdrop_args.get("pdrop_layers", [])
                    ]
                )
                self.alpha_ffn = nn.Parameter(
                    (torch.zeros(len(self.merge_ffn_modules)))
                )
            elif merge_module_name == "CrossAttention_wo_MLP":
                self.merge_module_names = [
                    "attention" for _ in self.pdrop_args.get("pdrop_layers", [])
                ]
                self.merge_modules = nn.ModuleList(
                    [
                        Qwen2VLSdpaCrossAttention(config, layer_idx=idx)
                        for idx in range(config.num_hidden_layers)
                        if idx in self.pdrop_args.get("pdrop_layers", [])
                    ]
                )
                self.alpha = nn.Parameter((torch.zeros(len(self.merge_modules))))
                self.merge_ffn_modules = None
                self.alpha_ffn = None
            elif merge_module_name == "no_merge":
                self.merge_modules = None
                self.alpha = None
                self.merge_ffn_modules = None
                self.alpha_ffn = None
            else:
                raise ValueError(
                    f"Invalid merge module name for Qwen2: {merge_module_name}"
                )

        # Initialize weights and apply final processing
        self.post_init()

    # pdrop: New method
    def merge_dropped_information(
        self,
        features,
        cur_num,
        vision_index,
        start_index,
        top_rank_index,
        dropped_index,
    ):
        """features[dropped_index, :]: num_dropped_token, hidden_size
        features[start_index:, :]: num_text_token, hidden_size
        """
        # merge the dropped information back
        if self.merge_module_names[cur_num] == "attention":
            merged_features = self.merge_modules[cur_num](
                features[start_index:, :].unsqueeze(0),
                features[dropped_index, :].unsqueeze(0),
            )[0].squeeze(0)
            new_text_features = (
                features[start_index:, :] + self.alpha[cur_num].tanh() * merged_features
            )
        else:
            raise ValueError(
                f"Unsupported merge module type: {self.merge_module_names[cur_num]}"
            )

        if self.alpha_ffn is not None:
            new_text_features = new_text_features + self.alpha_ffn[
                cur_num
            ].tanh() * self.merge_ffn_modules[cur_num](
                new_text_features,
            )
        return new_text_features

    # pdrop: New method
    def pdrop_no_pack(
        self,
        features,
        cur_num,
        rank_layer,
        pdrop_compress_type,
        labels,
        position_ids,
        attention_mask,
        first_vision_token_positions,
        num_vision_tokens,
        text_prompt_lens=None,
    ):
        _labels = labels
        _position_ids = position_ids
        _attention_mask = attention_mask
        image_tokens = [
            int(cur_vision_token * self.pdrop_ratios[cur_num])
            for cur_vision_token in num_vision_tokens
        ]
        keep_length = [
            int(cur_vision_token * self.pdrop_ratios[cur_num + 1])
            for cur_vision_token in num_vision_tokens
        ]
        batch_size = features.shape[0]

        features_list = []
        attention_mask_list = []
        labels_list = []

        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, features.shape[1]),
                dtype=torch.bool,
                device=features.device,
            )
        else:
            attention_mask = attention_mask.bool()
        if labels is None:
            labels = torch.full(
                (batch_size, features.shape[1]), -100, device=features.device
            )

        if "attn" in pdrop_compress_type:
            # obtain query_states and key_states to calculate attention map
            hidden_states = features.clone().detach()

            self_attn = self.layers[rank_layer].self_attn

            num_heads = self_attn.config.num_attention_heads
            num_key_value_heads = self_attn.config.num_key_value_heads
            head_dim = self_attn.head_dim

            bsz, q_len, _ = hidden_states.size()

            query_states = self_attn.q_proj(hidden_states)
            key_states = self_attn.k_proj(hidden_states)

            query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(
                1, 2
            )
            key_states = key_states.view(
                bsz, q_len, num_key_value_heads, head_dim
            ).transpose(1, 2)

            key_states = repeat_kv(key_states, self_attn.num_key_value_groups)

            eager_attention_mask = torch.zeros(
                q_len, q_len, dtype=query_states.dtype, device=query_states.device
            )
            temp_mask = torch.ones(
                q_len, q_len, dtype=torch.bool, device=query_states.device
            ).tril(diagonal=0)
            eager_attention_mask.masked_fill_(temp_mask.logical_not(), float("-inf"))
            eager_attention_mask = eager_attention_mask.unsqueeze(0).unsqueeze(
                0
            )  # (1, 1, q_len, q_len)

        # take valid features
        features = [
            cur_features[cur_attention_mask]
            for cur_features, cur_attention_mask in zip(features, attention_mask)
        ]
        labels = [
            cur_labels[cur_attention_mask]
            for cur_labels, cur_attention_mask in zip(labels, attention_mask)
        ]
        attention_mask = [
            cur_attention_mask[cur_attention_mask]
            for cur_attention_mask, cur_attention_mask in zip(
                attention_mask, attention_mask
            )
        ]

        # rank & drop
        for i in range(batch_size):
            vision_index = first_vision_token_positions[i]

            if "attn" in pdrop_compress_type:
                cur_key_states = key_states[i]
                cur_query_states = query_states[i]
                cur_eager_attention_mask = eager_attention_mask

                if self.training:
                    answer_index = torch.where(labels[i] != -100)[0].tolist()
                    index_before_answer = [
                        index - 1
                        for index in answer_index
                        if labels[i][index - 1] == -100
                    ]

                    if not index_before_answer:
                        features_list.append(features[i])
                        attention_mask_list.append(attention_mask[i])
                        labels_list.append(labels[i])
                        continue

                    index_before_answer = torch.tensor(
                        index_before_answer[-1], device=labels[i].device
                    )
                    text_query_states = cur_query_states[:, index_before_answer, :]
                    text_eager_attention_mask = cur_eager_attention_mask[
                        :, :, index_before_answer, :
                    ]
                else:
                    prompt_total_len = text_prompt_lens[i] + image_tokens[i]
                    text_query_states = cur_query_states[
                        :, prompt_total_len - 1, :
                    ].unsqueeze(1)
                    text_eager_attention_mask = cur_eager_attention_mask[
                        :, :, prompt_total_len - 1, :
                    ].unsqueeze(2)

                attn_weights = torch.matmul(
                    text_query_states, cur_key_states.transpose(1, 2)
                ) / math.sqrt(head_dim)
                attn_weights = attn_weights + text_eager_attention_mask
                attn_weights = nn.functional.softmax(
                    attn_weights, dim=-1, dtype=torch.float32
                ).to(query_states.dtype)
                attention_avg_head = torch.mean(attn_weights, dim=0)
                attention_avg_head = attention_avg_head[
                    :, vision_index : vision_index + image_tokens[i]
                ]
                attention_avg_text = torch.mean(attention_avg_head, dim=0)
                top_rank_index = attention_avg_text.topk(keep_length[i]).indices

            elif "uni" in pdrop_compress_type:
                top_rank_index = torch.linspace(
                    0,
                    image_tokens[i] - 1,
                    keep_length[i],
                    dtype=torch.long,
                    device=features[i].device,
                )
            else:
                raise NotImplementedError(pdrop_compress_type)

            top_rank_index = (top_rank_index + vision_index).sort().values
            start_index = vision_index + image_tokens[i]

            if self.merge_modules is not None:
                all_indices = torch.arange(
                    vision_index, start_index, device=top_rank_index.device
                )
                mask = ~torch.isin(all_indices, top_rank_index)
                dropped_index = all_indices[mask].tolist()
                text_features = self.merge_dropped_information(
                    features[i],
                    cur_num,
                    vision_index,
                    start_index,
                    top_rank_index,
                    dropped_index,
                )
            else:
                text_features = features[i][start_index:, :]

            new_input_embeds = torch.cat(
                [
                    features[i][:vision_index, :],
                    features[i][top_rank_index, :],
                    text_features,
                ],
                dim=0,
            )
            new_labels = torch.cat(
                [
                    labels[i][:vision_index],
                    labels[i][top_rank_index],
                    labels[i][start_index:],
                ],
                dim=0,
            )
            new_attention_mask = torch.cat(
                [
                    attention_mask[i][:vision_index],
                    attention_mask[i][top_rank_index],
                    attention_mask[i][start_index:],
                ],
                dim=0,
            )

            features_list.append(new_input_embeds)
            attention_mask_list.append(new_attention_mask)
            labels_list.append(new_labels)

        new_input_embeds = features_list
        new_attention_mask = attention_mask_list
        new_labels = labels_list
        if batch_size > 0:
            max_len = max(x.shape[0] for x in new_input_embeds)
            embeds_padded, labels_paded, attention_mask_padded = [], [], []
            new_position_ids = torch.zeros(
                (batch_size, max_len),
                dtype=position_ids.dtype,
                device=position_ids.device,
            )

            for i, (cur_new_embed, cur_new_labels, cur_new_mask) in enumerate(
                zip(new_input_embeds, new_labels, new_attention_mask)
            ):
                dif = max_len - cur_new_embed.shape[0]
                embeds_padded.append(
                    torch.nn.functional.pad(cur_new_embed, (0, 0, 0, dif))
                )
                labels_paded.append(
                    torch.nn.functional.pad(cur_new_labels, (0, dif), value=-100)
                )
                attention_mask_padded.append(
                    torch.nn.functional.pad(cur_new_mask, (0, dif), value=False)
                )
                new_position_ids[i, : cur_new_mask.shape[0]] = torch.arange(
                    0,
                    cur_new_mask.shape[0],
                    dtype=position_ids.dtype,
                    device=position_ids.device,
                )

            new_input_embeds = torch.stack(embeds_padded, dim=0).to(features[0].dtype)
            new_attention_mask = torch.stack(attention_mask_padded, dim=0)
            new_labels = torch.stack(labels_paded, dim=0)
            position_ids = new_position_ids

        if _labels is None:
            new_labels = None
        if _attention_mask is None:
            new_attention_mask = None
        else:
            new_attention_mask = new_attention_mask.to(dtype=_attention_mask.dtype)

        return position_ids, new_attention_mask, new_input_embeds, new_labels, None

    # pdrop: New method
    def pdrop_pack(
        self,
        packed_features: List[torch.Tensor],
        packed_position_ids: List[torch.Tensor],
        packed_labels: List[torch.Tensor],
        cur_num: int,
        rank_layer: int,
        pdrop_compress_type,
        seq_idx,
        train_pdrop_args,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        seq_idx_list, position_ids_list, labels_list, feature_list = [], [], [], []

        for idx, (
            feature,
            pos_id,
            num_vision_token,
            first_vision_token_position,
            label,
        ) in enumerate(
            zip(
                packed_features,
                packed_position_ids,
                train_pdrop_args["num_vision_tokens"],
                train_pdrop_args["first_vision_token_positions"],
                packed_labels,
            )
        ):
            pos_ids, new_attention_mask, new_input_embeds, new_labels, _ = (
                self.pdrop_no_pack(
                    feature.unsqueeze(0),
                    cur_num,
                    rank_layer,
                    pdrop_compress_type,
                    label.unsqueeze(0),
                    pos_id.unsqueeze(0),
                    None,
                    [first_vision_token_position],
                    [num_vision_token],
                )
            )
            token_len = pos_ids.shape[1]
            seq_idx_list.append(
                torch.full((1, token_len), idx, dtype=torch.int, device=feature.device)
            )
            position_ids_list.append(pos_ids)
            labels_list.append(new_labels)
            feature_list.append(new_input_embeds[0])

        seq_idx = torch.cat(seq_idx_list, dim=1)
        position_ids = torch.cat(position_ids_list, dim=1)
        labels = torch.cat(labels_list, dim=1)
        features = torch.cat(feature_list, dim=0).unsqueeze(0)
        train_pdrop_args["sample_seq_lens"] = [x.shape[0] for x in feature_list]

        return position_ids, new_attention_mask, features, labels, seq_idx

    # pdrop: New method
    def flash_rank_drop(
        self,
        cur_num,
        rank_layer,
        features,
        position_ids,
        attention_mask,
        labels,
        is_packed=False,
        seq_idx=None,
        train_pdrop_args=None,
    ):
        pdrop_compress_type = self.pdrop_args["pdrop_compress_types"][cur_num]
        if position_ids is None:
            position_ids = torch.arange(
                0, features.shape[1], dtype=torch.long, device=features.device
            ).unsqueeze(0)

        if is_packed:
            seq_lens = train_pdrop_args["sample_seq_lens"]
            features = torch.split(features.squeeze(0), seq_lens, dim=0)
            position_ids = torch.split(position_ids.squeeze(0), seq_lens, dim=0)
            labels = torch.split(labels.squeeze(0), seq_lens, dim=0)
            position_ids, new_attention_mask, new_input_embeds, new_labels, seq_idx = (
                self.pdrop_pack(
                    features,
                    position_ids,
                    labels,
                    cur_num,
                    rank_layer,
                    pdrop_compress_type,
                    seq_idx,
                    train_pdrop_args=train_pdrop_args,
                )
            )
        else:
            return self.pdrop_no_pack(
                features,
                cur_num,
                rank_layer,
                pdrop_compress_type,
                labels,
                position_ids,
                attention_mask,
                train_pdrop_args["first_vision_token_positions"],
                train_pdrop_args["num_vision_tokens"],
                train_pdrop_args.get("text_prompt_lens"),
            )
        return position_ids, new_attention_mask, new_input_embeds, new_labels, seq_idx

    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[BaseModelOutputWithPast, BaseModelOutputWithPastAndLabels]:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError(
                "You must specify exactly one of input_ids or inputs_embeds"
            )

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        use_cache = (
            use_cache
            if use_cache is not None
            else (self.config.use_cache if not self.training else False)
        )

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = (
                past_key_values.get_seq_length() if past_key_values is not None else 0
            )
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = (
                    create_sliding_window_causal_mask(**mask_kwargs)
                )

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)
        seq_idx = kwargs.get("seq_idx", None)

        for layer_idx, decoder_layer in enumerate(self.layers):
            # pdrop: Logic to drop tokens
            if self.use_pdrop and layer_idx in self.pdrop_args.get("pdrop_layers", []):
                stage = self.pdrop_args["pdrop_layers"].index(layer_idx)
                train_pdrop_args = kwargs.get("train_pdrop_args")

                if hidden_states.shape[1] != 1:  # Prefill or training
                    if train_pdrop_args is None:
                        raise ValueError(
                            "train_pdrop_args must be provided for pdrop during prefill/training."
                        )
                    is_packed = "sample_seq_lens" in train_pdrop_args
                    (
                        position_ids,
                        attention_mask,
                        hidden_states,
                        labels,
                        seq_idx,
                    ) = self.flash_rank_drop(
                        cur_num=stage,
                        rank_layer=layer_idx,
                        features=hidden_states,
                        position_ids=position_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                        is_packed=is_packed,
                        seq_idx=seq_idx,
                        train_pdrop_args=train_pdrop_args,
                    )
                    # After dropping tokens, sequence length, etc. have changed. Recompute masks and embeddings.
                    cache_position = torch.arange(
                        hidden_states.shape[1], device=hidden_states.device
                    )
                    mask_kwargs = {
                        "config": self.config,
                        "input_embeds": hidden_states,
                        "attention_mask": attention_mask,
                        "cache_position": cache_position,
                        "past_key_values": None,
                        "position_ids": position_ids,
                    }
                    causal_mask_mapping = {
                        "full_attention": create_causal_mask(**mask_kwargs)
                    }
                    if self.has_sliding_layers:
                        causal_mask_mapping["sliding_attention"] = (
                            create_sliding_window_causal_mask(**mask_kwargs)
                        )
                    position_embeddings = self.rotary_emb(hidden_states, position_ids)

                else:  # Decoding
                    num_vision_tokens = train_pdrop_args["num_vision_tokens"]
                    cur_visual_length = [
                        int(vt * self.pdrop_ratios[stage]) for vt in num_vision_tokens
                    ]
                    next_visual_length = [
                        int(vt * self.pdrop_ratios[stage + 1])
                        for vt in num_vision_tokens
                    ]

                    new_position_ids = [
                        pid - (cur_visual_length[i] - next_visual_length[i])
                        for i, pid in enumerate(position_ids.flatten().tolist())
                    ]
                    position_ids = torch.tensor(
                        new_position_ids, dtype=torch.long, device=position_ids.device
                    ).view_as(position_ids)

            hidden_states = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)
        self.sample_seq_lens = None
        if self.use_pdrop:
            return BaseModelOutputWithPastAndLabels(
                last_hidden_state=hidden_states,
                past_key_values=past_key_values if use_cache else None,
                labels=labels,
                hidden_states=None,  # Not returning these to match source
                attentions=None,  # Not returning these to match source
            )
        else:
            return BaseModelOutputWithPast(
                last_hidden_state=hidden_states,
                past_key_values=past_key_values if use_cache else None,
            )


@auto_docstring
class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen2Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        r"""
        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen2ForCausalLM

        >>> model = Qwen2ForCausalLM.from_pretrained("meta-qwen2/Qwen2-2-7b-hf")
        >>> tokenizer = AutoTokenizer.from_pretrained("meta-qwen2/Qwen2-2-7b-hf")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # pdrop: If pdrop is used, labels might have been updated.
        if self.model.use_pdrop and labels is not None:
            labels = outputs.labels

        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = (
            slice(-logits_to_keep, None)
            if isinstance(logits_to_keep, int)
            else logits_to_keep
        )
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(
                shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1)
            )

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def set_pdrop_args(self, **kwargs):
        """æŠŠå‚æ•°ä¼ é€’åˆ° model é‡Œ"""
        for key, value in kwargs.items():
            setattr(self.model, key, value)

    # pdrop: New method
    def init_cross_attn_from_self_attn(self):
        if (
            hasattr(self.model, "merge_modules")
            and self.model.merge_modules is not None
        ):
            for idx, module in enumerate(self.model.merge_modules):
                pdrop_layers = self.model.pdrop_args.get("pdrop_layers", [])
                try:
                    module.load_state_dict(
                        self.model.layers[pdrop_layers[idx]].self_attn.state_dict()
                    )
                except Exception as e:
                    print(f"idx: {idx}", e)

    # pdrop: New method
    def init_merge_modules_from_nearest_self_attn(self):
        if (
            hasattr(self.model, "merge_modules")
            and self.model.merge_modules is not None
        ):
            for idx, module in enumerate(self.model.merge_modules):
                pdrop_layers = self.model.pdrop_args.get("pdrop_layers", [])
                for layer_idx in range(pdrop_layers[idx], pdrop_layers[idx] + 20):
                    if layer_idx < len(self.model.layers):
                        try:
                            module.load_state_dict(
                                self.model.layers[layer_idx].self_attn.state_dict()
                            )
                            break
                        except Exception as e:
                            print(
                                f"failed to init merge module {idx} from layer {layer_idx}:",
                                e,
                            )

    # pdrop: New method
    def init_mlp_from_llm(self):
        if (
            hasattr(self.model, "merge_ffn_modules")
            and self.model.merge_ffn_modules is not None
        ):
            for idx, module in enumerate(self.model.merge_ffn_modules):
                pdrop_layers = self.model.pdrop_args.get("pdrop_layers", [])
                for layer_idx in range(pdrop_layers[idx], pdrop_layers[idx] + 4):
                    if layer_idx < len(self.model.layers):
                        try:
                            print(
                                "loading MLP weights for merge module from layer:",
                                layer_idx,
                            )
                            module.load_state_dict(
                                self.model.layers[layer_idx].mlp.state_dict()
                            )
                            break
                        except Exception as e:
                            print(
                                f"failed to init merge ffn {idx} from layer {layer_idx}:",
                                e,
                            )


class Qwen2ForSequenceClassification(
    GenericForSequenceClassification, Qwen2PreTrainedModel
):
    pass


class Qwen2ForTokenClassification(GenericForTokenClassification, Qwen2PreTrainedModel):
    pass


class Qwen2ForQuestionAnswering(GenericForQuestionAnswering, Qwen2PreTrainedModel):
    base_model_prefix = (
        "transformer"  # For BC, where `transformer` was used instead of `model`
    )


__all__ = [
    "Qwen2PreTrainedModel",
    "Qwen2Model",
    "Qwen2ForCausalLM",
    "Qwen2RMSNorm",
    "Qwen2ForSequenceClassification",
    "Qwen2ForTokenClassification",
    "Qwen2ForQuestionAnswering",
]

AutoConfig.register("qwen2", Qwen2Config, exist_ok=True)
AutoModelForCausalLM.register(Qwen2Config, Qwen2ForCausalLM, exist_ok=True)
